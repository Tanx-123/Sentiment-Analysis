{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.txt\", delimiter=';', header=None, names=['sentence','label'])\n",
    "df_test = pd.read_csv(\"test.txt\", delimiter=';', header=None, names=['sentence','label'])\n",
    "df_val = pd.read_csv(\"val.txt\", delimiter=';', header=None, names=['sentence','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train,df_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['label_encoded'] = labelencoder.fit_transform(df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>im having ssa examination tomorrow in the morn...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>i constantly worry about their fight against n...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>i feel its important to share this info for th...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>i truly feel that if you are passionate enough...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>i feel like i just wanna buy any cute make up ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence    label  \\\n",
       "0                               i didnt feel humiliated  sadness   \n",
       "1     i can go from feeling so hopeless to so damned...  sadness   \n",
       "2      im grabbing a minute to post i feel greedy wrong    anger   \n",
       "3     i am ever feeling nostalgic about the fireplac...     love   \n",
       "4                                  i am feeling grouchy    anger   \n",
       "...                                                 ...      ...   \n",
       "1995  im having ssa examination tomorrow in the morn...  sadness   \n",
       "1996  i constantly worry about their fight against n...      joy   \n",
       "1997  i feel its important to share this info for th...      joy   \n",
       "1998  i truly feel that if you are passionate enough...      joy   \n",
       "1999  i feel like i just wanna buy any cute make up ...      joy   \n",
       "\n",
       "      label_encoded  \n",
       "0                 4  \n",
       "1                 4  \n",
       "2                 0  \n",
       "3                 3  \n",
       "4                 0  \n",
       "...             ...  \n",
       "1995              4  \n",
       "1996              2  \n",
       "1997              2  \n",
       "1998              2  \n",
       "1999              2  \n",
       "\n",
       "[18000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surprise</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  label_encoded\n",
       "0   sadness              4\n",
       "2     anger              0\n",
       "3      love              3\n",
       "6  surprise              5\n",
       "7      fear              1\n",
       "8       joy              2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['label','label_encoded']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence         object\n",
       "label            object\n",
       "label_encoded     int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = df['sentence'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         6066\n",
       "sadness     5216\n",
       "anger       2434\n",
       "fear        2149\n",
       "love        1482\n",
       "surprise     653\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ids = np.zeros((len(df), max_seqlen))\n",
    "X_mask = np.zeros((len(df), max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sequence in enumerate(df['sentence']):\n",
    "    token = tokenizer.encode_plus(sequence, \n",
    "                              add_special_tokens=True,\n",
    "                              max_length=max_seqlen, \n",
    "                              truncation=True,\n",
    "                              padding=\"max_length\",\n",
    "                              return_token_type_ids=False,\n",
    "                              return_attention_mask=True,\n",
    "                              return_tensors='tf')\n",
    "    X_ids[i,:], X_mask[i,: ] = token['input_ids'],token['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101.,   178.,  1238., ...,     0.,     0.,     0.],\n",
       "       [  101.,   178.,  1169., ...,     0.,     0.,     0.],\n",
       "       [  101., 13280., 10810., ...,     0.,     0.,     0.],\n",
       "       ...,\n",
       "       [  101.,   178.,  1631., ...,     0.,     0.,     0.],\n",
       "       [  101.,   178.,  5098., ...,     0.,     0.,     0.],\n",
       "       [  101.,   178.,  1631., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr=df['label_encoded'].values\n",
    "arr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros((arr.size,arr.max()+1))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.arange(arr.size),arr] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_ids,X_mask,labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([  101.,   178.,  1238.,  1204.,  1631., 21820., 21896.,   102.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.])>, <tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, <tf.Tensor: shape=(6,), dtype=float64, numpy=array([0., 0., 0., 0., 1., 0.])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(input_ids,attention_mask,labels):\n",
    "    return{'input_ids':input_ids,'attention_mask':attention_mask},labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(map_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=({'input_ids': TensorSpec(shape=(50,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(50,), dtype=tf.float64, name=None)}, TensorSpec(shape=(6,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([  101.,   178.,  1238.,  1204.,  1631., 21820., 21896.,   102.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(6,), dtype=float64, numpy=array([0., 0., 0., 0., 1., 0.])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 50), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(None, 50), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 6), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_batches = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.take(round(no_of_batches * 0.9))\n",
    "validation = dataset.skip(round(no_of_batches * 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "bert = TFAutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(max_seqlen,), name=\"input_ids\",dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(max_seqlen,), name=\"attention_mask\",dtype='int32')\n",
    "\n",
    "embedding = bert(input_ids, attention_mask=mask)[0]\n",
    "X = tf.keras.layers.GlobalMaxPooling1D()(embedding)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "y = tf.keras.layers.Dense(6, activation='softmax',name='outputs')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 6)            774         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,412,550\n",
      "Trainable params: 108,411,014\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 6)            774         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,412,550\n",
      "Trainable params: 100,742\n",
      "Non-trainable params: 108,311,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 162s 266ms/step - loss: 1.4054 - accuracy: 0.4800 - val_loss: 1.0437 - val_accuracy: 0.6002\n",
      "Epoch 2/100\n",
      "507/507 [==============================] - 129s 254ms/step - loss: 1.1910 - accuracy: 0.5486 - val_loss: 0.9716 - val_accuracy: 0.6425\n",
      "Epoch 3/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 1.1297 - accuracy: 0.5713 - val_loss: 0.9147 - val_accuracy: 0.6577\n",
      "Epoch 4/100\n",
      "507/507 [==============================] - 128s 253ms/step - loss: 1.1031 - accuracy: 0.5812 - val_loss: 0.8907 - val_accuracy: 0.6695\n",
      "Epoch 5/100\n",
      "507/507 [==============================] - 143s 281ms/step - loss: 1.0815 - accuracy: 0.5912 - val_loss: 0.8843 - val_accuracy: 0.6661\n",
      "Epoch 6/100\n",
      "507/507 [==============================] - 149s 293ms/step - loss: 1.0704 - accuracy: 0.5912 - val_loss: 0.8401 - val_accuracy: 0.6914\n",
      "Epoch 7/100\n",
      "507/507 [==============================] - 151s 299ms/step - loss: 1.0494 - accuracy: 0.5992 - val_loss: 0.8572 - val_accuracy: 0.6892\n",
      "Epoch 8/100\n",
      "507/507 [==============================] - 144s 284ms/step - loss: 1.0284 - accuracy: 0.6137 - val_loss: 0.7882 - val_accuracy: 0.7078\n",
      "Epoch 9/100\n",
      "507/507 [==============================] - 144s 284ms/step - loss: 1.0166 - accuracy: 0.6143 - val_loss: 0.7791 - val_accuracy: 0.6965\n",
      "Epoch 10/100\n",
      "507/507 [==============================] - 144s 284ms/step - loss: 1.0034 - accuracy: 0.6167 - val_loss: 0.7105 - val_accuracy: 0.7455\n",
      "Epoch 11/100\n",
      "507/507 [==============================] - 131s 259ms/step - loss: 0.9975 - accuracy: 0.6190 - val_loss: 0.7209 - val_accuracy: 0.7303\n",
      "Epoch 12/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.9807 - accuracy: 0.6288 - val_loss: 0.6924 - val_accuracy: 0.7528\n",
      "Epoch 13/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.9669 - accuracy: 0.6317 - val_loss: 0.7213 - val_accuracy: 0.7393\n",
      "Epoch 14/100\n",
      "507/507 [==============================] - 133s 262ms/step - loss: 0.9556 - accuracy: 0.6353 - val_loss: 0.6463 - val_accuracy: 0.7663\n",
      "Epoch 15/100\n",
      "507/507 [==============================] - 132s 260ms/step - loss: 0.9426 - accuracy: 0.6424 - val_loss: 0.6697 - val_accuracy: 0.7449\n",
      "Epoch 16/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.9401 - accuracy: 0.6405 - val_loss: 0.6307 - val_accuracy: 0.7663\n",
      "Epoch 17/100\n",
      "507/507 [==============================] - 132s 260ms/step - loss: 0.9315 - accuracy: 0.6443 - val_loss: 0.6254 - val_accuracy: 0.7736\n",
      "Epoch 18/100\n",
      "507/507 [==============================] - 132s 260ms/step - loss: 0.9262 - accuracy: 0.6513 - val_loss: 0.6212 - val_accuracy: 0.7759\n",
      "Epoch 19/100\n",
      "507/507 [==============================] - 137s 270ms/step - loss: 0.9168 - accuracy: 0.6508 - val_loss: 0.6044 - val_accuracy: 0.7984\n",
      "Epoch 20/100\n",
      "507/507 [==============================] - 150s 296ms/step - loss: 0.8972 - accuracy: 0.6580 - val_loss: 0.5694 - val_accuracy: 0.7934\n",
      "Epoch 21/100\n",
      "507/507 [==============================] - 151s 298ms/step - loss: 0.9053 - accuracy: 0.6580 - val_loss: 0.5430 - val_accuracy: 0.8063\n",
      "Epoch 22/100\n",
      "507/507 [==============================] - 147s 289ms/step - loss: 0.8940 - accuracy: 0.6609 - val_loss: 0.5367 - val_accuracy: 0.8097\n",
      "Epoch 23/100\n",
      "507/507 [==============================] - 137s 269ms/step - loss: 0.8748 - accuracy: 0.6652 - val_loss: 0.5466 - val_accuracy: 0.8007\n",
      "Epoch 24/100\n",
      "507/507 [==============================] - 121s 239ms/step - loss: 0.8726 - accuracy: 0.6694 - val_loss: 0.5624 - val_accuracy: 0.7962\n",
      "Epoch 25/100\n",
      "507/507 [==============================] - 124s 244ms/step - loss: 0.8666 - accuracy: 0.6666 - val_loss: 0.5066 - val_accuracy: 0.8232\n",
      "Epoch 26/100\n",
      "507/507 [==============================] - 132s 261ms/step - loss: 0.8694 - accuracy: 0.6725 - val_loss: 0.5321 - val_accuracy: 0.8193\n",
      "Epoch 27/100\n",
      "507/507 [==============================] - 120s 237ms/step - loss: 0.8537 - accuracy: 0.6714 - val_loss: 0.5183 - val_accuracy: 0.8221\n",
      "Epoch 28/100\n",
      "507/507 [==============================] - 120s 237ms/step - loss: 0.8496 - accuracy: 0.6738 - val_loss: 0.4710 - val_accuracy: 0.8474\n",
      "Epoch 29/100\n",
      "507/507 [==============================] - 120s 237ms/step - loss: 0.8515 - accuracy: 0.6743 - val_loss: 0.4583 - val_accuracy: 0.8446\n",
      "Epoch 30/100\n",
      "507/507 [==============================] - 121s 239ms/step - loss: 0.8519 - accuracy: 0.6746 - val_loss: 0.4923 - val_accuracy: 0.8221\n",
      "Epoch 31/100\n",
      "507/507 [==============================] - 120s 237ms/step - loss: 0.8460 - accuracy: 0.6796 - val_loss: 0.4923 - val_accuracy: 0.8384\n",
      "Epoch 32/100\n",
      "507/507 [==============================] - 139s 274ms/step - loss: 0.8398 - accuracy: 0.6795 - val_loss: 0.4741 - val_accuracy: 0.8452\n",
      "Epoch 33/100\n",
      "507/507 [==============================] - 151s 298ms/step - loss: 0.8433 - accuracy: 0.6802 - val_loss: 0.4805 - val_accuracy: 0.8350\n",
      "Epoch 34/100\n",
      "507/507 [==============================] - 151s 297ms/step - loss: 0.8335 - accuracy: 0.6839 - val_loss: 0.4791 - val_accuracy: 0.8294\n",
      "Epoch 35/100\n",
      "507/507 [==============================] - 142s 280ms/step - loss: 0.8304 - accuracy: 0.6832 - val_loss: 0.4610 - val_accuracy: 0.8519\n",
      "Epoch 36/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.8335 - accuracy: 0.6807 - val_loss: 0.4408 - val_accuracy: 0.8497\n",
      "Epoch 37/100\n",
      "507/507 [==============================] - 132s 259ms/step - loss: 0.8265 - accuracy: 0.6916 - val_loss: 0.4446 - val_accuracy: 0.8474\n",
      "Epoch 38/100\n",
      "507/507 [==============================] - 132s 259ms/step - loss: 0.8215 - accuracy: 0.6871 - val_loss: 0.4322 - val_accuracy: 0.8553\n",
      "Epoch 39/100\n",
      "507/507 [==============================] - 134s 264ms/step - loss: 0.8184 - accuracy: 0.6884 - val_loss: 0.4217 - val_accuracy: 0.8660\n",
      "Epoch 40/100\n",
      "507/507 [==============================] - 132s 261ms/step - loss: 0.8102 - accuracy: 0.6934 - val_loss: 0.4046 - val_accuracy: 0.8654\n",
      "Epoch 41/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.8201 - accuracy: 0.6866 - val_loss: 0.4531 - val_accuracy: 0.8390\n",
      "Epoch 42/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.8088 - accuracy: 0.6971 - val_loss: 0.4440 - val_accuracy: 0.8412\n",
      "Epoch 43/100\n",
      "507/507 [==============================] - 131s 257ms/step - loss: 0.8020 - accuracy: 0.6969 - val_loss: 0.4273 - val_accuracy: 0.8542\n",
      "Epoch 44/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.8118 - accuracy: 0.6940 - val_loss: 0.4071 - val_accuracy: 0.8637\n",
      "Epoch 45/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.8075 - accuracy: 0.6894 - val_loss: 0.3720 - val_accuracy: 0.8784\n",
      "Epoch 46/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.8002 - accuracy: 0.6939 - val_loss: 0.4043 - val_accuracy: 0.8677\n",
      "Epoch 47/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.8031 - accuracy: 0.6963 - val_loss: 0.4101 - val_accuracy: 0.8575\n",
      "Epoch 48/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7984 - accuracy: 0.6976 - val_loss: 0.3875 - val_accuracy: 0.8722\n",
      "Epoch 49/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.7910 - accuracy: 0.7015 - val_loss: 0.3919 - val_accuracy: 0.8739\n",
      "Epoch 50/100\n",
      "507/507 [==============================] - 129s 255ms/step - loss: 0.7942 - accuracy: 0.7002 - val_loss: 0.3700 - val_accuracy: 0.8767\n",
      "Epoch 51/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.7834 - accuracy: 0.7030 - val_loss: 0.3725 - val_accuracy: 0.8778\n",
      "Epoch 52/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7943 - accuracy: 0.7001 - val_loss: 0.3756 - val_accuracy: 0.8823\n",
      "Epoch 53/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7825 - accuracy: 0.7045 - val_loss: 0.3818 - val_accuracy: 0.8677\n",
      "Epoch 54/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.7818 - accuracy: 0.7035 - val_loss: 0.3715 - val_accuracy: 0.8784\n",
      "Epoch 55/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7867 - accuracy: 0.7000 - val_loss: 0.3515 - val_accuracy: 0.8964\n",
      "Epoch 56/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7829 - accuracy: 0.7008 - val_loss: 0.3563 - val_accuracy: 0.8868\n",
      "Epoch 57/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7795 - accuracy: 0.7043 - val_loss: 0.3785 - val_accuracy: 0.8795\n",
      "Epoch 58/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7790 - accuracy: 0.7032 - val_loss: 0.3504 - val_accuracy: 0.8857\n",
      "Epoch 59/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7700 - accuracy: 0.7086 - val_loss: 0.3505 - val_accuracy: 0.8896\n",
      "Epoch 60/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7771 - accuracy: 0.7011 - val_loss: 0.3601 - val_accuracy: 0.8857\n",
      "Epoch 61/100\n",
      "507/507 [==============================] - 131s 257ms/step - loss: 0.7848 - accuracy: 0.7009 - val_loss: 0.3472 - val_accuracy: 0.8925\n",
      "Epoch 62/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7775 - accuracy: 0.7075 - val_loss: 0.3428 - val_accuracy: 0.8953\n",
      "Epoch 63/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7634 - accuracy: 0.7090 - val_loss: 0.3324 - val_accuracy: 0.8981\n",
      "Epoch 64/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7668 - accuracy: 0.7132 - val_loss: 0.3415 - val_accuracy: 0.8846\n",
      "Epoch 65/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7624 - accuracy: 0.7087 - val_loss: 0.3247 - val_accuracy: 0.8941\n",
      "Epoch 66/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7671 - accuracy: 0.7104 - val_loss: 0.3450 - val_accuracy: 0.8953\n",
      "Epoch 67/100\n",
      "507/507 [==============================] - 129s 255ms/step - loss: 0.7574 - accuracy: 0.7133 - val_loss: 0.3183 - val_accuracy: 0.8947\n",
      "Epoch 68/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7564 - accuracy: 0.7136 - val_loss: 0.3378 - val_accuracy: 0.8947\n",
      "Epoch 69/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7636 - accuracy: 0.7098 - val_loss: 0.3462 - val_accuracy: 0.8958\n",
      "Epoch 70/100\n",
      "507/507 [==============================] - 130s 255ms/step - loss: 0.7593 - accuracy: 0.7136 - val_loss: 0.3252 - val_accuracy: 0.8936\n",
      "Epoch 71/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7542 - accuracy: 0.7165 - val_loss: 0.3181 - val_accuracy: 0.9082\n",
      "Epoch 72/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7528 - accuracy: 0.7149 - val_loss: 0.3492 - val_accuracy: 0.9032\n",
      "Epoch 73/100\n",
      "507/507 [==============================] - 130s 256ms/step - loss: 0.7553 - accuracy: 0.7114 - val_loss: 0.3123 - val_accuracy: 0.9184\n",
      "Epoch 74/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.7485 - accuracy: 0.7161 - val_loss: 0.3174 - val_accuracy: 0.9020\n",
      "Epoch 75/100\n",
      "507/507 [==============================] - 131s 258ms/step - loss: 0.7560 - accuracy: 0.7147 - val_loss: 0.3091 - val_accuracy: 0.9110\n",
      "Epoch 76/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7497 - accuracy: 0.7161 - val_loss: 0.2790 - val_accuracy: 0.9184\n",
      "Epoch 77/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7508 - accuracy: 0.7146 - val_loss: 0.3308 - val_accuracy: 0.8986\n",
      "Epoch 78/100\n",
      "507/507 [==============================] - 132s 260ms/step - loss: 0.7371 - accuracy: 0.7179 - val_loss: 0.2822 - val_accuracy: 0.9167\n",
      "Epoch 79/100\n",
      "507/507 [==============================] - 133s 262ms/step - loss: 0.7573 - accuracy: 0.7143 - val_loss: 0.3114 - val_accuracy: 0.9082\n",
      "Epoch 80/100\n",
      "507/507 [==============================] - 134s 264ms/step - loss: 0.7507 - accuracy: 0.7149 - val_loss: 0.2969 - val_accuracy: 0.9071\n",
      "Epoch 81/100\n",
      "507/507 [==============================] - 135s 265ms/step - loss: 0.7444 - accuracy: 0.7218 - val_loss: 0.3047 - val_accuracy: 0.9065\n",
      "Epoch 82/100\n",
      "507/507 [==============================] - 135s 266ms/step - loss: 0.7447 - accuracy: 0.7194 - val_loss: 0.3054 - val_accuracy: 0.9105\n",
      "Epoch 83/100\n",
      "507/507 [==============================] - 132s 261ms/step - loss: 0.7356 - accuracy: 0.7224 - val_loss: 0.3236 - val_accuracy: 0.9026\n",
      "Epoch 84/100\n",
      "507/507 [==============================] - 130s 257ms/step - loss: 0.7520 - accuracy: 0.7169 - val_loss: 0.3024 - val_accuracy: 0.9217\n",
      "Epoch 85/100\n",
      "507/507 [==============================] - 128s 252ms/step - loss: 0.7291 - accuracy: 0.7263 - val_loss: 0.3060 - val_accuracy: 0.8998\n",
      "Epoch 86/100\n",
      "507/507 [==============================] - 128s 251ms/step - loss: 0.7428 - accuracy: 0.7191 - val_loss: 0.3165 - val_accuracy: 0.8992\n",
      "Epoch 87/100\n",
      "507/507 [==============================] - 127s 251ms/step - loss: 0.7327 - accuracy: 0.7244 - val_loss: 0.2978 - val_accuracy: 0.9093\n",
      "Epoch 88/100\n",
      "507/507 [==============================] - 127s 251ms/step - loss: 0.7353 - accuracy: 0.7253 - val_loss: 0.2936 - val_accuracy: 0.9161\n",
      "Epoch 89/100\n",
      "507/507 [==============================] - 127s 251ms/step - loss: 0.7275 - accuracy: 0.7214 - val_loss: 0.2951 - val_accuracy: 0.9110\n",
      "Epoch 90/100\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.7359 - accuracy: 0.7221 - val_loss: 0.2848 - val_accuracy: 0.9139\n",
      "Epoch 91/100\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.7347 - accuracy: 0.7222 - val_loss: 0.2955 - val_accuracy: 0.9240\n",
      "Epoch 92/100\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.7342 - accuracy: 0.7242 - val_loss: 0.2970 - val_accuracy: 0.9167\n",
      "Epoch 93/100\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.7327 - accuracy: 0.7229 - val_loss: 0.2943 - val_accuracy: 0.9144\n",
      "Epoch 94/100\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.7330 - accuracy: 0.7251 - val_loss: 0.2824 - val_accuracy: 0.9116\n",
      "Epoch 95/100\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.7287 - accuracy: 0.7207 - val_loss: 0.2987 - val_accuracy: 0.9088\n",
      "Epoch 96/100\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.7315 - accuracy: 0.7202 - val_loss: 0.2830 - val_accuracy: 0.9184\n",
      "Epoch 97/100\n",
      "507/507 [==============================] - 126s 248ms/step - loss: 0.7265 - accuracy: 0.7273 - val_loss: 0.2794 - val_accuracy: 0.9217\n",
      "Epoch 98/100\n",
      "507/507 [==============================] - 126s 248ms/step - loss: 0.7279 - accuracy: 0.7271 - val_loss: 0.2986 - val_accuracy: 0.9065\n",
      "Epoch 99/100\n",
      "507/507 [==============================] - 126s 248ms/step - loss: 0.7327 - accuracy: 0.7232 - val_loss: 0.2922 - val_accuracy: 0.9144\n",
      "Epoch 100/100\n",
      "507/507 [==============================] - 126s 248ms/step - loss: 0.7366 - accuracy: 0.7159 - val_loss: 0.3004 - val_accuracy: 0.9054\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=validation,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sentiment_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
